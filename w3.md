`w3.d1`
## Linear Regression

Lecture Feedback: review quiz answers for first 10-15 mins in morning

Dot product:
If xâ‹…y=0 then x and y are orthogonal (aligns with the intuitive notion of perpendicular).

The stochastic matrix is central to the Markov process. It is a square matrix specifying that probabilities of going from one state to the other such that every column of the matrix sums to 1.

----------
`w3.d2`
## Linear Regression cont'd

R^2 = 1 - (fraction of unexplained variance)
    = 1 - sum(model deviations) / sum(data deviations)
    = 1 - sum(y - yhat)^2 / sum(y-ybar)^2

The reason why we div by y-ybar is bc it's the naive, simple model.

Leverage: the 'pull' of an unusual x value.
How much yhat changes according to a change in y.
e.g. if y goes up for a data point clustered in the middle of a group of points, the line won't move much. On the other hand, if y goes up for a data point that's far out on the right, a slight increase in y can 'pull' up the regression line.

### Outliers
How to identify them? (box plots don't you anything about the outliers of your MODEL)
- Plot studentized residuals vs predicted y
What do you do after identifying them?
- Drop them
- Log transform your data to bring them closer
- Gather more observations

### Multicollinearity
How to identify?
- Calculate VIF (Variance Inflation Factor) for each variable. If >10, it's collinear with other variable(s)
- Visually inspect using scatter_matrix

Interaction terms: if two variables have synergistic effects
Non-linear transformation: if data doesn't follow linear pattern
Y-variable transform: when data is too clustered together

----------
`w3.d3`
## Regularized Regression

When you add a feature to linear regression, R^2 will ALWAYS go up. So it's not necessarily a reliable metric (bc you can keep adding until you get 0.99)

In general we don't know if something if under-fit

Regularization: Ridge/Lasso
Increasing lambda increases your bias, while decreasing your variance. This is awesome! Helps resolve issues of overfitting.

----------
`w3.d4`
## Logistic Regression

Logit function: s shaped curve from the sigmoid family
"Link" function
Assuming the y is binomial, and x is a logit function

All generalized linear models have 3 characteristics:
1) A probability distribution describing the outcome variable
2) A linear model
3) A link function that relates the linear model to the parameter of the outcome distribution

logit(p) = log(p/1-p) = log(ODDS)
The Logit function takes a value [0,1] and maps it to a value [-infinity, infinity]

g^-1(x) = e^x / (1+e^x)
The Inverse Logit function takes a value [-infinity, infinity] and maps it to a value [0,1]

### Confusion Matrix
Accuracy = (TP+TN / totals)
#### Recall (or Sensitivity)
- TP / (TP+FN)
- How often you label it positive, when it is actually positive (This is same as True Positive Rate)
#### Specificity
- TN / (TN+FP)
- How often you label it negative, when it is actually negative
- (False Positive Rate is just 1-Specificity)
#### Precision
- TP / (TP+FP)
- How often you GET it right, when you call positive

ROC curve is only based on rank-ordering. As long as the ordering doesn't change, the ROC curve will be the same.

----------
`w3.d5`
## Gradient Descent

The whole idea of what we've been doing so far is minimizing a cost function

For linear regression, there exists a closed form solution
beta = (X.t X)^-1 X.t y
For logistic regression, unfortunately there isn't...

Likelihood function

Gradient Descent:
- Gradient descent/ascent is an algorithm for solving optimization problems. In gradient descent, we would like to find the solution which minimizes the cost function. In gradient ascent (which is the case of logistic regression), we will be maximizing the likelihood.
- Example: searching for camp in a dark forest (camp is located at the bottom of the valley)
- Important note: we need to know that there is only one minimum point. If there are several local minima, we could end up at a wrong point.

If your cost function is increasing (diverging), then alpha might be too large.

Termination
- Repeat until the decrease in cost function is sufficiently small
