`w4.d1`
## Class Imbalance & Profit Curves

When we have class imbalance, we can either do Cost-Sensitive Learning, or use Sampling Techniques to reduce the imbalance.

COST SENSITIVE LEARNING
In a confusion matrix,
E(profit) = sum of net benefits weighted to each cell's probability

We can plot a profit curve (where x is portion of test instances predicted to be positive, y is profit). The maximum point in the curve is where we should choose our threshold.

SAMPLING
How to deal with class imbalance using Sampling?
- Undersampling: new dataset will have a length of min(class A, class B). e.g. randomly select len(class A) amount for class B.

- Oversampling: new dataset will have a length of max(class A, class B). e.g. sample with replacement from class A until we have the same amount as class B.

SMOTE (Synthetic Minority Oversampling Technique)
- Generates synthetic observations using interpolation from minority class

----------
## Web Scraping + MongoDB

mongod runs the server, mongo opens up the shell

```
db.log.find({$or: [{a: /Mozilla/}, {a: /Opera/}]}).count()
```
This returns a count for all the entries where the 'a' key contains like-phrases of Mozilla or Opera.

```
db.log.findOne({'t': {$exists: true}}).g
```
This returns the 'g' attribute (or the value for the 'g' key) in one random entry where that entry has a 't' attribute

```
db.log.find({'t': {$exists: true}}).forEach(function(entry) {
  db.log.update({_id: entry._id}, {$set: {t: entry.t*1000}})
  db.log.update({_id: entry._id}, {$set: {t: new Date(entry.t)}})
})
```
This writes a function that goes through each entry where a value exists for the key 't', and does the following:
- set t to t*1000
- set t to a new Date object

```
db.log.find({'t': {$exists: true}}, {t: true}).sort({t: 1}).limit(10)
```
Inside the find(), the first part states the condition (a value exists for the key 't'), while the second part states what variables you want to show (like SELECT in SQL).

----------
## NLP + Naive Bayes

Recall Bayes: P(H|E) = P(E|H) * P(H) / P(E)

MAP Estimation
Maximum a posteriori Estimation
- Find H to maximize P(H|E)
- We can 'ignore' the denominator because the evidence is constant (already happened/assumed).
- So we can just look at the numerator: P(E|H) * P(H)

When to use Naive Bayes?
- When we have more features than rows (sparse data)
-

PIPELINE
- Every object passed into sklearn's pipeline must have a fit and a transform method.
- Fit must return self

----------
