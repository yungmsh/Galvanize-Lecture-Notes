`w4.d1`
## K Nearest Neighbors

General idea: when predicting the classification of a particular point, choose its k nearest neighbors, and have some rule to determine what class it should take (majority vote, average, etc.).
It is a non-parametric model.

K Nearest Neighbors is computationally expensive, because it has to go through all your data points to make one prediction.
However, there isn't really a 'training' process. Having your data set saved is already having it 'trained'.

You can draw these arbitrary decision boundaries to see what class your hypothetical data point would be.

Be careful with categorical variables. Works weird with distance metrics.
Distance metrics: Euclidean, Manhattan, Cosine
Cosine distance tends to do better in higher dimension spaces.

#### Choosing k
- As you increase k, it becomes more like a linear regression. As a result, you reduce the variance (don't model to the noise). Reduce overfitting.
- Use CV. Plot k against an error metric, and find the 'elbow' point.

#### Curse of Dimensionality
- As the number of dimensions increases, the amount of 'space' reaches infinity. (The ratio of hypersphere to hypercube goes to 0)
- Basically, data points are either really close to one another, or REALLY far from one another. (think of it like every feature becomes a binary variable)
- This can causes K Nearest Neighbors to break down.
- So don't use it if you have 10+ predictors.

## Decision Trees
- Non-parametric, can model non-linear relationships
- Interaction effects are naturally encoded into the way the model is set up
- Computationally cheap to predict
- Computationally expensive to train
- Easily overfits
- Low bias -> high variance (can be problematic...)

Basic idea: compute Information Gain (using Shannon Entropy equation) at every node to determine which feature we should use to split.
Information gain: Parent's Entropy - sum of Children's Entropy (weighted)
We choose the split that yields the greatest information gain
Alternatively, we can use Gini Index instead of entropy.

#### Regression Trees
Instead of using information gain, we can just calculate the variance.

#### Pruning
Reducing overfit (variance), because of a perfect decision tree essentially 'memorizes' 100% of your data.
Pre-pruning: Leaf size, depth, purity, gain threshold
Post-pruning: Merge leaves if doing so decreases test-error

----------

## Random Forests, Bagging, Boosting

Bagging = Bootstrap Aggregation
- Take a bunch of bootstrap samples n
- Train model on each of them
- Average the results (this can reduce variance by sqrt(n), but in effect the reduction is less)

Subspace Sampling: at each decision tree, split only m = sqrt(p) features
=> Random forests improve on bagging by de-correlating the trees

Random Forests
- Great performance
- No feature scaling needed
- Can model non-linear relationships
- Expensive to train (but can be done in parallel!)
- Not really interpretable (compared to a single decision tree)

How do we get around the lack of interpretability?
- Mean decrease in impurity (Gini for classification, MSE for regression)

It is highly the case that Random Forest Regressor will yield a lower MSE than Linear Regression.

----------

## SVM
Difference between Logistic and SVM is that Logistic takes into account every data point, while SVM only uses the 'support vectors' to build the hyperplane.

----------

## Boosting

Reduce variance by building our model slowly over time.
We start with a weak learner to avoid overfitting (from the start).

f(x) = f0(x) + f1(x) + f2(x) + ... fmax(x)

e.g.
1. Start with the mean
2. Poor predictive ability.. now calculate the residuals
3. Treat the residuals as a new data set
4. Build a simple tree (where residuals are the target output for the new equation)
5. Keep doing that (residuals will start falling).
6. IMPORTANT: instead of adding the entirety of the residual, we add some small fraction of it (lambda).

Parameter Tuning:
- Max depth (4 to 6)
- Min samples per Leaf
- n estimators
- Learning rate
- Max features
- subsample (random subset every iteration)

#### Partial Dependency Plot
Good way of understanding the importance of a variable.
Procedure:
1) Replace entire column with current val that's being cycled over
2) Predict y for all rows. Calculate the mean.
3) Compare this mean to the mean of the last iteration.
4) This difference is the (value, difference) pair in the partial dependency plot.
This can be done in 3D as well (hold var1 constant, loop through var2. Then, hold var2 constant, loop through var1)

#### AdaBoost
Adaptive Boosting model
Changing the weights as we go.
The model focuses on the points that it misclassifies as it progresses.
The weighting is cumulative (if you get a datapoint right a few times, its importance shrinks)
