## PCA and Clustering

Notes from textbook:
Both clustering and PCA seek to simplify the data via a small number
of summaries, but their mechanisms are different:
- PCA looks to find a low-dimensional representation of the observations
that explain a good fraction of the variance;
- Clustering looks to find homogeneous subgroups among the observations.

Questions:
- Why doesn't logistic L1 normalization set coefficients to 0?
- why is cos(theta) = d/|x|

When to use PCA:
- kNN on high dim data
- Clustering on high dim data
- Visualization (embedding)
- Works well with images
BUT
- Loses interpretability (the new components aren't 'real' features)
- If your model doesn't need reduction, don't do it.

-----------

## Recommenders

The basic goal is to fill in the missing values in the utility matrix (e.g. users vs items)

If your utility matrix contains negative values, you might want to scale it so it's positive.

Item-item similarity: choose some similarity metric
- Cosine
- Correlation
- Jaccard works better with binary data (listened or did not listen to the song). Ratio = (A and B) / (A or B)

Prediction:
- To predict a user's missing item A rating, we take the weighted average of all other items' ratings, weighted to their similarity with item A.
- In some cases, we don't want to look at all the other items (most are probably dissimilar), so we might choose the top N most similar items. (This is known as Neighboring)

Cold start problem:
- Most affected: Item-item
- Moderately affected: SVD (latent factors)
- Least affected: observable factors, popularity

As you increase k (the number of topics), the RMSE goes down (because you're decomposing the matrix at a finer level). But if you increase k too much:
- It can be computationally expensive
- It can overfit
- Topics lose interpretability

Validation Metrics:
- RMSE
We do a train/test split and predict on the test. This can be highly problematic because test data will not a random sample of all possible values! In fact, the non-missing data we have to begin with is highly biased. Also, we care more about the high ratings but RMSE weights them all equally. (i.e. screwing up the low ratings matters less than screwing up the high ratings)
- Precision at n
We take the top n predicted items, and calculate the % of those which are relevant (i.e. has been rated by that user)

The one way to truly validate a recommender is by using an A/B test.
- Compare your recommender against a randomized recommender, or a standardized recommender
